
2025-01-23 14:53:40,018 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=64 seq_num = 50, gap = 3, --------------
2025-01-23 14:53:47,135 - INFO - Accuracy on the test data = [86.42821907997131]
2025-01-23 14:53:47,166 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=64 seq_num = 50, gap = 5, --------------
2025-01-23 14:53:53,399 - INFO - Accuracy on the test data = [83.2925021648407]
2025-01-23 14:53:53,437 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=64 seq_num = 80, gap = 3, --------------
2025-01-23 14:54:00,659 - INFO - Accuracy on the test data = [87.265545129776]
2025-01-23 14:54:00,692 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=64 seq_num = 80, gap = 5, --------------
2025-01-23 14:54:05,881 - INFO - Accuracy on the test data = [84.79763269424438]
2025-01-23 14:54:05,924 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=64 seq_num = 100, gap = 3, --------------
2025-01-23 14:54:11,330 - INFO - Accuracy on the test data = [87.84722089767456]
2025-01-23 14:54:11,367 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=64 seq_num = 100, gap = 5, --------------
2025-01-23 14:54:16,695 - INFO - Accuracy on the test data = [86.3095223903656]
2025-01-23 14:54:16,732 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=128 seq_num = 50, gap = 3, --------------
2025-01-23 14:54:23,319 - INFO - Accuracy on the test data = [82.80254602432251]
2025-01-23 14:54:23,352 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=128 seq_num = 50, gap = 5, --------------
2025-01-23 14:54:29,012 - INFO - Accuracy on the test data = [84.07643437385559]
2025-01-23 14:54:29,052 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=128 seq_num = 80, gap = 3, --------------
2025-01-23 14:54:34,501 - INFO - Accuracy on the test data = [87.61105537414551]
2025-01-23 14:54:34,537 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=128 seq_num = 80, gap = 5, --------------
2025-01-23 14:54:39,599 - INFO - Accuracy on the test data = [88.79565596580505]
2025-01-23 14:54:39,641 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=128 seq_num = 100, gap = 3, --------------
2025-01-23 14:54:44,439 - INFO - Accuracy on the test data = [88.88888955116272]
2025-01-23 14:54:44,476 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=128 seq_num = 100, gap = 5, --------------
2025-01-23 14:54:50,048 - INFO - Accuracy on the test data = [87.20238208770752]
2025-01-23 14:58:30,967 - INFO - numLayer1_hiddenSize64_seq_num50_gap3_accuracy86.43, total samples: 5449
2025-01-23 14:58:30,969 - INFO - 
 [[726  75  56  17   6   0  15]
 [136 630 203  76  16  11   0]
 [122  57 645  86   0   0   0]
 [ 63  22  91 458   0  17  10]
 [  0   0   8   0 547  15   0]
 [ 26   0   2   6   7 719   3]
 [  0   0  22   0  11 145 400]]
2025-01-23 14:58:30,976 - INFO - 
               precision    recall  f1-score   support

           1       0.68      0.81      0.74       895
           2       0.80      0.59      0.68      1072
           3       0.63      0.71      0.67       910
           4       0.71      0.69      0.70       661
           5       0.93      0.96      0.95       570
           6       0.79      0.94      0.86       763
           7       0.93      0.69      0.80       578

    accuracy                           0.76      5449
   macro avg       0.78      0.77      0.77      5449
weighted avg       0.77      0.76      0.76      5449

2025-01-23 14:58:30,981 - INFO - numLayer1_hiddenSize64_seq_num80_gap3_accuracy87.27, total samples: 5411
2025-01-23 14:58:30,982 - INFO - 
 [[740  58  58  19  15   0   0]
 [ 49 666 290   0   0  54   7]
 [ 22  98 743  46   5   2   5]
 [ 25   7 126 405  12  88   0]
 [  9   0   0   1 547   0   0]
 [ 24   9   8  12   0 698   0]
 [ 13   0   4  10   0 130 406]]
2025-01-23 14:58:30,990 - INFO - 
               precision    recall  f1-score   support

           1       0.84      0.83      0.84       890
           2       0.79      0.62      0.70      1066
           3       0.60      0.81      0.69       921
           4       0.82      0.61      0.70       663
           5       0.94      0.98      0.96       557
           6       0.72      0.93      0.81       751
           7       0.97      0.72      0.83       563

    accuracy                           0.78      5411
   macro avg       0.81      0.79      0.79      5411
weighted avg       0.80      0.78      0.78      5411

2025-01-23 14:58:30,995 - INFO - numLayer1_hiddenSize64_seq_num100_gap3_accuracy87.85, total samples: 5375
2025-01-23 14:58:30,995 - INFO - 
 [[739  65  50  11  18   0   0]
 [ 89 689 241  20  19   0   0]
 [ 17  89 747  35   1  29   0]
 [  0   0   4 602  29  29   0]
 [  0   0   0   0 543   0   2]
 [ 20   9   0  24   0 651  41]
 [  0   0   0   0   7  76 479]]
2025-01-23 14:58:31,002 - INFO - 
               precision    recall  f1-score   support

           1       0.85      0.84      0.85       883
           2       0.81      0.65      0.72      1058
           3       0.72      0.81      0.76       918
           4       0.87      0.91      0.89       664
           5       0.88      1.00      0.93       545
           6       0.83      0.87      0.85       745
           7       0.92      0.85      0.88       562

    accuracy                           0.83      5375
   macro avg       0.84      0.85      0.84      5375
weighted avg       0.83      0.83      0.83      5375

2025-01-23 14:58:31,008 - INFO - numLayer1_hiddenSize64_seq_num50_gap5_accuracy83.29, total samples: 5442
2025-01-23 14:58:31,009 - INFO - 
 [[666 138  65   7   2   1  18]
 [132 624 277  23  13   4   0]
 [ 60 112 706  30   0   0   0]
 [ 14  85  85 438  19  17   0]
 [  0   9   3   0 557   0   0]
 [ 23   2  15  28   7 683   3]
 [ 20   1  11   0   0 105 439]]
2025-01-23 14:58:31,016 - INFO - 
               precision    recall  f1-score   support

           1       0.73      0.74      0.74       897
           2       0.64      0.58      0.61      1073
           3       0.61      0.78      0.68       908
           4       0.83      0.67      0.74       658
           5       0.93      0.98      0.95       569
           6       0.84      0.90      0.87       761
           7       0.95      0.76      0.85       576

    accuracy                           0.76      5442
   macro avg       0.79      0.77      0.78      5442
weighted avg       0.77      0.76      0.76      5442

2025-01-23 14:58:31,022 - INFO - numLayer1_hiddenSize64_seq_num80_gap5_accuracy84.80, total samples: 5398
2025-01-23 14:58:31,022 - INFO - 
 [[690 116  80   6   0   0   0]
 [ 86 595 321  13   0  35  14]
 [ 77  83 691  57   6   0   6]
 [  0   9  94 444  40  75   0]
 [  0   0   0   1 535  19   0]
 [ 12  12   0  55   0 661   7]
 [  4   0  17  27   0 149 361]]
2025-01-23 14:58:31,029 - INFO - 
               precision    recall  f1-score   support

           1       0.79      0.77      0.78       892
           2       0.73      0.56      0.63      1064
           3       0.57      0.75      0.65       920
           4       0.74      0.67      0.70       662
           5       0.92      0.96      0.94       555
           6       0.70      0.88      0.78       747
           7       0.93      0.65      0.76       558

    accuracy                           0.74      5398
   macro avg       0.77      0.75      0.75      5398
weighted avg       0.75      0.74      0.74      5398

2025-01-23 14:58:31,034 - INFO - numLayer1_hiddenSize64_seq_num100_gap5_accuracy86.31, total samples: 5375
2025-01-23 14:58:31,035 - INFO - 
 [[711  78  32  30  30   0   0]
 [122 653 247  11  22   0   5]
 [ 29  95 739  26  29   2   0]
 [  0   0  69 536  27  21   9]
 [  0   0   0   0 547   0   0]
 [ 19  17   7  43   0 657   0]
 [  5   0   0   0   2 126 429]]
2025-01-23 14:58:31,042 - INFO - 
               precision    recall  f1-score   support

           1       0.80      0.81      0.80       881
           2       0.77      0.62      0.69      1060
           3       0.68      0.80      0.73       920
           4       0.83      0.81      0.82       662
           5       0.83      1.00      0.91       547
           6       0.82      0.88      0.85       743
           7       0.97      0.76      0.85       562

    accuracy                           0.79      5375
   macro avg       0.81      0.81      0.81      5375
weighted avg       0.80      0.79      0.79      5375

2025-01-23 14:58:31,047 - INFO - numLayer1_hiddenSize128_seq_num50_gap3_accuracy82.80, total samples: 5471
2025-01-23 14:58:31,047 - INFO - 
 [[652  98  75  13  26  24  13]
 [ 98 701 221  27  23  10   0]
 [ 37 181 681  18   0   0   0]
 [ 44   9  89 473  25  16   5]
 [  0  22  10   0 529   9   0]
 [ 18  12  19   2   1 706   7]
 [  2   0  23   0   0 148 404]]
2025-01-23 14:58:31,055 - INFO - 
               precision    recall  f1-score   support

           1       0.77      0.72      0.74       901
           2       0.69      0.65      0.67      1080
           3       0.61      0.74      0.67       917
           4       0.89      0.72      0.79       661
           5       0.88      0.93      0.90       570
           6       0.77      0.92      0.84       765
           7       0.94      0.70      0.80       577

    accuracy                           0.76      5471
   macro avg       0.79      0.77      0.77      5471
weighted avg       0.77      0.76      0.76      5471

2025-01-23 14:58:31,060 - INFO - numLayer1_hiddenSize128_seq_num80_gap3_accuracy87.61, total samples: 5410
2025-01-23 14:58:31,060 - INFO - 
 [[745  65  57  22   0   0   0]
 [ 66 624 363   6   0   8   0]
 [ 54  90 741  17   9   9   2]
 [ 18  45  82 519   0   0   0]
 [  0   0   0   0 556   0   0]
 [ 17   1  14  47   0 669   2]
 [ 12   0  14   0   7  82 447]]
2025-01-23 14:58:31,067 - INFO - 
               precision    recall  f1-score   support

           1       0.82      0.84      0.83       889
           2       0.76      0.58      0.66      1067
           3       0.58      0.80      0.68       922
           4       0.85      0.78      0.81       664
           5       0.97      1.00      0.99       556
           6       0.87      0.89      0.88       750
           7       0.99      0.80      0.88       562

    accuracy                           0.80      5410
   macro avg       0.83      0.81      0.82      5410
weighted avg       0.81      0.80      0.80      5410

2025-01-23 14:58:31,073 - INFO - numLayer1_hiddenSize128_seq_num100_gap3_accuracy88.89, total samples: 5375
2025-01-23 14:58:31,073 - INFO - 
 [[763  63  48   7   0   0   0]
 [ 75 755 202   0  24   0   0]
 [ 25  69 802  10  11   2   0]
 [  0 130  62 471   0   0   0]
 [  0   0   0   0 548   0   0]
 [ 18  19   9  33   0 665   0]
 [  0   0   0   7   1 151 405]]
2025-01-23 14:58:31,080 - INFO - 
               precision    recall  f1-score   support

           1       0.87      0.87      0.87       881
           2       0.73      0.71      0.72      1056
           3       0.71      0.87      0.79       919
           4       0.89      0.71      0.79       663
           5       0.94      1.00      0.97       548
           6       0.81      0.89      0.85       744
           7       1.00      0.72      0.84       564

    accuracy                           0.82      5375
   macro avg       0.85      0.83      0.83      5375
weighted avg       0.83      0.82      0.82      5375

2025-01-23 14:58:31,086 - INFO - numLayer1_hiddenSize128_seq_num50_gap5_accuracy84.08, total samples: 5439
2025-01-23 14:58:31,087 - INFO - 
 [[679  89  53   9  48   8  16]
 [243 587 154  21  50   3  16]
 [113 163 582  26   0   0  21]
 [ 10   0 136 446  52  10   0]
 [  0  18   5   5 541   0   1]
 [ 20   0   0  28   6 663  40]
 [ 16   0  13   0  15  73 460]]
2025-01-23 14:58:31,094 - INFO - 
               precision    recall  f1-score   support

           1       0.63      0.75      0.68       902
           2       0.68      0.55      0.61      1074
           3       0.62      0.64      0.63       905
           4       0.83      0.68      0.75       654
           5       0.76      0.95      0.84       570
           6       0.88      0.88      0.88       757
           7       0.83      0.80      0.81       577

    accuracy                           0.73      5439
   macro avg       0.75      0.75      0.74      5439
weighted avg       0.73      0.73      0.73      5439

2025-01-23 14:58:31,099 - INFO - numLayer1_hiddenSize128_seq_num80_gap5_accuracy88.80, total samples: 5385
2025-01-23 14:58:31,100 - INFO - 
 [[767  48  69   3   0   0   0]
 [112 605 275  62   6   1   2]
 [ 61  95 655 101   4   2   0]
 [ 13  16  63 454  34  83   0]
 [  0   0  23  25 472  32   0]
 [ 20   7   0  65   4 648   0]
 [ 17   4  11   0   0 116 410]]
2025-01-23 14:58:31,107 - INFO - 
               precision    recall  f1-score   support

           1       0.77      0.86      0.82       887
           2       0.78      0.57      0.66      1063
           3       0.60      0.71      0.65       918
           4       0.64      0.68      0.66       663
           5       0.91      0.86      0.88       552
           6       0.73      0.87      0.80       744
           7       1.00      0.73      0.85       558

    accuracy                           0.74      5385
   macro avg       0.78      0.76      0.76      5385
weighted avg       0.76      0.74      0.74      5385

2025-01-23 14:58:31,112 - INFO - numLayer1_hiddenSize128_seq_num100_gap5_accuracy87.20, total samples: 5364
2025-01-23 14:58:31,113 - INFO - 
 [[729  57  46  20  15   1  12]
 [106 601 305  16  28   0   0]
 [ 24  99 692  76  20   0   7]
 [ 27   0  55 511  16  53   0]
 [  0   0   0  15 533   0   0]
 [ 15  14  12  55  16 626   1]
 [ 17   0   0   6   1  78 459]]
2025-01-23 14:58:31,121 - INFO - 
               precision    recall  f1-score   support

           1       0.79      0.83      0.81       880
           2       0.78      0.57      0.66      1056
           3       0.62      0.75      0.68       918
           4       0.73      0.77      0.75       662
           5       0.85      0.97      0.91       548
           6       0.83      0.85      0.84       739
           7       0.96      0.82      0.88       561

    accuracy                           0.77      5364
   macro avg       0.79      0.79      0.79      5364
weighted avg       0.78      0.77      0.77      5364

---------------------------------------------------------------------------------------------------------------------------------------
Trained on franka_main tested on franka_mindlab

2025-01-23 16:14:19,383 - INFO - numLayer1_hiddenSize64_seq_num50_gap3_accuracy80.28, total samples: 5458
2025-01-23 16:14:19,384 - INFO - 
 [[ 10  65  81  77 374 225  65]
 [  0  51  41  92 572 222 103]
 [  0  57  60 105 546  51  96]
 [  0   0  13 234 198 156  57]
 [  0   0   0   0 440 130   0]
 [  0   0   0   2  85 582  93]
 [  0   0   0   0  59 251 265]]
2025-01-23 16:14:19,392 - INFO - 
               precision    recall  f1-score   support

           1       1.00      0.01      0.02       897
           2       0.29      0.05      0.08      1081
           3       0.31      0.07      0.11       915
           4       0.46      0.36      0.40       658
           5       0.19      0.77      0.31       570
           6       0.36      0.76      0.49       762
           7       0.39      0.46      0.42       575

    accuracy                           0.30      5458
   macro avg       0.43      0.35      0.26      5458
weighted avg       0.44      0.30      0.23      5458

2025-01-23 16:14:19,397 - INFO - numLayer1_hiddenSize64_seq_num80_gap3_accuracy86.38, total samples: 5402
2025-01-23 16:14:19,397 - INFO - 
 [[ 26   9  23  35 493 265  40]
 [  0  17  37  53 635 282  44]
 [  0  33  22  52 553 203  54]
 [  0   0   0  34 400 204  22]
 [  0   0   1   0 413 137   0]
 [  0   0   0  24  20 657  51]
 [  0   0   0   0  36 155 372]]
2025-01-23 16:14:19,404 - INFO - 
               precision    recall  f1-score   support

           1       1.00      0.03      0.06       891
           2       0.29      0.02      0.03      1068
           3       0.27      0.02      0.04       917
           4       0.17      0.05      0.08       660
           5       0.16      0.75      0.27       551
           6       0.35      0.87      0.49       752
           7       0.64      0.66      0.65       563

    accuracy                           0.29      5402
   macro avg       0.41      0.34      0.23      5402
weighted avg       0.42      0.29      0.20      5402

2025-01-23 16:14:19,409 - INFO - numLayer1_hiddenSize64_seq_num100_gap3_accuracy91.30, total samples: 5378
2025-01-23 16:14:19,410 - INFO - 
 [[106  61  66   6 387 215  38]
 [  0 110  28  65 444 344  67]
 [  0  62  28  72 432 264  61]
 [  0  11  44  62 191 345  10]
 [  0   0   0   0 305 243   0]
 [  0   0   0  12   0 733   0]
 [  0   0   0   0  20 162 384]]
2025-01-23 16:14:19,416 - INFO - 
               precision    recall  f1-score   support

           1       1.00      0.12      0.22       879
           2       0.45      0.10      0.17      1058
           3       0.17      0.03      0.05       919
           4       0.29      0.09      0.14       663
           5       0.17      0.56      0.26       548
           6       0.32      0.98      0.48       745
           7       0.69      0.68      0.68       566

    accuracy                           0.32      5378
   macro avg       0.44      0.37      0.29      5378
weighted avg       0.45      0.32      0.26      5378

2025-01-23 16:14:19,421 - INFO - numLayer1_hiddenSize64_seq_num50_gap5_accuracy81.53, total samples: 5456
2025-01-23 16:14:19,421 - INFO - 
 [[ 34  41  64  90 321 304  43]
 [  0  56  67  60 492 368  35]
 [  0  35  63 107 443 192  71]
 [  0   0  16  73 450  94  28]
 [  0   0   0   0 359 209   0]
 [  0   0   0   0  44 634  86]
 [  0   0   0   0  45 199 333]]
2025-01-23 16:14:19,429 - INFO - 
               precision    recall  f1-score   support

           1       1.00      0.04      0.07       897
           2       0.42      0.05      0.09      1078
           3       0.30      0.07      0.11       911
           4       0.22      0.11      0.15       661
           5       0.17      0.63      0.26       568
           6       0.32      0.83      0.46       764
           7       0.56      0.58      0.57       577

    accuracy                           0.28      5456
   macro avg       0.43      0.33      0.25      5456
weighted avg       0.45      0.28      0.22      5456

2025-01-23 16:14:19,434 - INFO - numLayer1_hiddenSize64_seq_num80_gap5_accuracy88.76, total samples: 5408
2025-01-23 16:14:19,434 - INFO - 
 [[ 38  24  25  30 352 311 110]
 [  0  94  30  38 481 361  62]
 [  0 123  23  33 455 206  78]
 [  0   4   0  79 284 240  56]
 [  0   0   0   0 357 199   0]
 [  0   0   0  13  17 708  13]
 [  0   0   0   0  40 217 307]]
2025-01-23 16:14:19,441 - INFO - 
               precision    recall  f1-score   support

           1       1.00      0.04      0.08       890
           2       0.38      0.09      0.14      1066
           3       0.29      0.03      0.05       918
           4       0.41      0.12      0.18       663
           5       0.18      0.64      0.28       556
           6       0.32      0.94      0.47       751
           7       0.49      0.54      0.52       564

    accuracy                           0.30      5408
   macro avg       0.44      0.34      0.25      5408
weighted avg       0.45      0.30      0.22      5408

2025-01-23 16:14:19,446 - INFO - numLayer1_hiddenSize64_seq_num100_gap5_accuracy91.70, total samples: 5374
2025-01-23 16:14:19,447 - INFO - 
 [[ 86   0 139  54 456  87  56]
 [  0 103  60 147 427 214 106]
 [ 12  65 128  96 420 114  86]
 [  0   0  77 159 227 170  28]
 [  0   0   0   0 341 165  40]
 [  0  12  12  17  11 692   0]
 [  0   0   0   0   8 189 370]]
2025-01-23 16:14:19,454 - INFO - 
               precision    recall  f1-score   support

           1       0.88      0.10      0.18       878
           2       0.57      0.10      0.17      1057
           3       0.31      0.14      0.19       921
           4       0.34      0.24      0.28       661
           5       0.18      0.62      0.28       546
           6       0.42      0.93      0.58       744
           7       0.54      0.65      0.59       567

    accuracy                           0.35      5374
   macro avg       0.46      0.40      0.32      5374
weighted avg       0.48      0.35      0.30      5374

2025-01-23 16:14:19,459 - INFO - numLayer1_hiddenSize128_seq_num50_gap3_accuracy80.28, total samples: 5442
2025-01-23 16:14:19,460 - INFO - 
 [[ 63  55 168  70 310 212  18]
 [  9  60 148  75 620 152  12]
 [ 15  67 223  94 407  42  62]
 [  0   0   0 270 208 179   0]
 [  0   0   0   0 449 119   0]
 [  0   0   2   3  76 617  59]
 [  0   0   0   0  63 185 330]]
2025-01-23 16:14:19,467 - INFO - 
               precision    recall  f1-score   support

           1       0.72      0.07      0.13       896
           2       0.33      0.06      0.10      1076
           3       0.41      0.25      0.31       910
           4       0.53      0.41      0.46       657
           5       0.21      0.79      0.33       568
           6       0.41      0.82      0.55       757
           7       0.69      0.57      0.62       578

    accuracy                           0.37      5442
   macro avg       0.47      0.42      0.36      5442
weighted avg       0.47      0.37      0.32      5442

2025-01-23 16:14:19,472 - INFO - numLayer1_hiddenSize128_seq_num80_gap3_accuracy90.74, total samples: 5407
2025-01-23 16:14:19,473 - INFO - 
 [[  8   0  68  10 495 202 105]
 [  1  62  15  44 532 328  85]
 [  0  32  71  29 510 223  57]
 [  0   0  25  29 301 309   0]
 [  0   0   0   0 341 214   0]
 [  0   0   0  12  44 670  24]
 [  0   0   0   0  38 200 323]]
2025-01-23 16:14:19,480 - INFO - 
               precision    recall  f1-score   support

           1       0.89      0.01      0.02       888
           2       0.66      0.06      0.11      1067
           3       0.40      0.08      0.13       922
           4       0.23      0.04      0.07       664
           5       0.15      0.61      0.24       555
           6       0.31      0.89      0.46       750
           7       0.54      0.58      0.56       561

    accuracy                           0.28      5407
   macro avg       0.46      0.32      0.23      5407
weighted avg       0.49      0.28      0.20      5407

2025-01-23 16:14:19,485 - INFO - numLayer1_hiddenSize128_seq_num100_gap3_accuracy91.62, total samples: 5375
2025-01-23 16:14:19,486 - INFO - 
 [[125  15  85  18 424 148  66]
 [  4  66  37   0 562 281 106]
 [ 30  66  94  60 458 133  79]
 [  0   0  39  44 268 306   8]
 [  0   0   0   0 354 174  20]
 [  0   5   0   6  24 685  21]
 [  0   0   0   0  52 146 366]]
2025-01-23 16:14:19,493 - INFO - 
               precision    recall  f1-score   support

           1       0.79      0.14      0.24       881
           2       0.43      0.06      0.11      1056
           3       0.37      0.10      0.16       920
           4       0.34      0.07      0.11       665
           5       0.17      0.65      0.26       548
           6       0.37      0.92      0.52       741
           7       0.55      0.65      0.60       564

    accuracy                           0.32      5375
   macro avg       0.43      0.37      0.29      5375
weighted avg       0.44      0.32      0.26      5375

2025-01-23 16:14:19,499 - INFO - numLayer1_hiddenSize128_seq_num50_gap5_accuracy77.86, total samples: 5455
2025-01-23 16:14:19,499 - INFO - 
 [[ 46  32  84  43 350 240  96]
 [  0  73 131  61 508 274  36]
 [  0  88  97  72 519  81  54]
 [  0   0  41 237 260 106  19]
 [  0   0   0   0 471  99   0]
 [  0   0   0   0  65 639  57]
 [  0   0   0   0  79 223 274]]
2025-01-23 16:14:19,506 - INFO - 
               precision    recall  f1-score   support

           1       1.00      0.05      0.10       891
           2       0.38      0.07      0.11      1083
           3       0.27      0.11      0.15       911
           4       0.57      0.36      0.44       663
           5       0.21      0.83      0.33       570
           6       0.38      0.84      0.53       761
           7       0.51      0.48      0.49       576

    accuracy                           0.34      5455
   macro avg       0.48      0.39      0.31      5455
weighted avg       0.48      0.34      0.28      5455

2025-01-23 16:14:19,511 - INFO - numLayer1_hiddenSize128_seq_num80_gap5_accuracy90.34, total samples: 5408
2025-01-23 16:14:19,512 - INFO - 
 [[ 41 145 140   7 338 191  29]
 [  0  99  35  45 457 382  44]
 [  0  76  27  37 500 194  89]
 [  0   0  10  25 303 288  36]
 [  0   0   1   0 333 211  10]
 [  0   0   2  12  16 677  42]
 [  0   0   0   0  36 183 347]]
2025-01-23 16:14:19,519 - INFO - 
               precision    recall  f1-score   support

           1       1.00      0.05      0.09       891
           2       0.31      0.09      0.14      1062
           3       0.13      0.03      0.05       923
           4       0.20      0.04      0.06       662
           5       0.17      0.60      0.26       555
           6       0.32      0.90      0.47       749
           7       0.58      0.61      0.60       566

    accuracy                           0.29      5408
   macro avg       0.39      0.33      0.24      5408
weighted avg       0.39      0.29      0.21      5408

2025-01-23 16:14:19,524 - INFO - numLayer1_hiddenSize128_seq_num100_gap5_accuracy90.90, total samples: 5372
2025-01-23 16:14:19,525 - INFO - 
 [[123  52 150  23 415  87  30]
 [  0 116  16  19 651 205  50]
 [ 13  20   4  26 561 253  40]
 [  0  16  72  99 168 308   0]
 [  0   0   0   0 358 169  21]
 [  0  13   0  17  20 691   0]
 [  0   0   0   0  48 211 307]]
2025-01-23 16:14:19,532 - INFO - 
               precision    recall  f1-score   support

           1       0.90      0.14      0.24       880
           2       0.53      0.11      0.18      1057
           3       0.02      0.00      0.01       917
           4       0.54      0.15      0.23       663
           5       0.16      0.65      0.26       548
           6       0.36      0.93      0.52       741
           7       0.69      0.54      0.61       566

    accuracy                           0.32      5372
   macro avg       0.46      0.36      0.29      5372
weighted avg       0.46      0.32      0.27      5372
--------------------------------------------------------------------------------------

-------------------------------------------------------------------------------------------------------------------------------
2025-01-23 16:46:27,429 - INFO - fine-tuning on pickleDatasets_0_2franka_mindlab with 7 links
2025-01-23 16:46:27,477 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=64 seq_num = 50, gap = 3, --------------
2025-01-23 16:46:36,052 - INFO - Accuracy on the test data = [87.55512237548828]
2025-01-23 16:46:36,097 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=64 seq_num = 80, gap = 3, --------------
2025-01-23 16:46:44,656 - INFO - Accuracy on the test data = [89.7334635257721]
2025-01-23 16:46:44,702 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=64 seq_num = 100, gap = 3, --------------
2025-01-23 16:46:53,406 - INFO - Accuracy on the test data = [90.17857313156128]
2025-01-23 16:46:53,443 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=64 seq_num = 50, gap = 5, --------------
2025-01-23 16:46:59,231 - INFO - Accuracy on the test data = [87.94708251953125]
2025-01-23 16:46:59,268 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=64 seq_num = 80, gap = 5, --------------
2025-01-23 16:47:05,076 - INFO - Accuracy on the test data = [88.35142850875854]
2025-01-23 16:47:05,114 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=64 seq_num = 100, gap = 5, --------------
2025-01-23 16:47:11,109 - INFO - Accuracy on the test data = [90.8730149269104]
2025-01-23 16:47:11,151 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=128 seq_num = 50, gap = 3, --------------
2025-01-23 16:47:20,005 - INFO - Accuracy on the test data = [88.68201971054077]
2025-01-23 16:47:20,051 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=128 seq_num = 80, gap = 3, --------------
2025-01-23 16:47:28,383 - INFO - Accuracy on the test data = [88.40078711509705]
2025-01-23 16:47:28,432 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=128 seq_num = 100, gap = 3, --------------
2025-01-23 16:47:36,787 - INFO - Accuracy on the test data = [91.71627163887024]
2025-01-23 16:47:36,825 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=128 seq_num = 50, gap = 5, --------------
2025-01-23 16:47:42,601 - INFO - Accuracy on the test data = [89.36795592308044]
2025-01-23 16:47:42,640 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=128 seq_num = 80, gap = 5, --------------
2025-01-23 16:47:48,433 - INFO - Accuracy on the test data = [90.96742272377014]
2025-01-23 16:47:48,474 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=128 seq_num = 100, gap = 5, --------------
2025-01-23 16:47:54,385 - INFO - Accuracy on the test data = [92.36111044883728]
2025-01-23 16:48:57,082 - INFO - Fine-tuned source model with franka_mindlab, tested on franka_mindlab, with 7 links
2025-01-23 16:51:40,344 - INFO - numLayer1_hiddenSize64_seq_num50_gap3_accuracy87.56, total samples: 5469
2025-01-23 16:51:40,345 - INFO - 
 [[751  51  22  44  23   0  11]
 [128 755 116  53  19   5   0]
 [ 67 127 716   0   7   0   0]
 [ 22   0  36 591  12   0   0]
 [  0   0   0   6 558   0   4]
 [ 15   0   0   5   7 739   0]
 [  0   4   5   0  10 134 426]]
2025-01-23 16:51:40,352 - INFO - 
               precision    recall  f1-score   support

           1       0.76      0.83      0.80       902
           2       0.81      0.70      0.75      1076
           3       0.80      0.78      0.79       917
           4       0.85      0.89      0.87       661
           5       0.88      0.98      0.93       568
           6       0.84      0.96      0.90       766
           7       0.97      0.74      0.84       579

    accuracy                           0.83      5469
   macro avg       0.84      0.84      0.84      5469
weighted avg       0.83      0.83      0.83      5469

2025-01-23 16:51:40,357 - INFO - numLayer1_hiddenSize64_seq_num80_gap3_accuracy89.73, total samples: 5396
2025-01-23 16:51:40,357 - INFO - 
 [[790  11  43  15   0  15  11]
 [114 692 171  72  13   0   1]
 [ 68 119 728   0   0   0   3]
 [  0   0  30 598  29   6   0]
 [  0   0   0   0 555   0   0]
 [  3   5  10  38   1 693   0]
 [ 10   0   0   0   4 126 422]]
2025-01-23 16:51:40,364 - INFO - 
               precision    recall  f1-score   support

           1       0.80      0.89      0.84       885
           2       0.84      0.65      0.73      1063
           3       0.74      0.79      0.77       918
           4       0.83      0.90      0.86       663
           5       0.92      1.00      0.96       555
           6       0.82      0.92      0.87       750
           7       0.97      0.75      0.84       562

    accuracy                           0.83      5396
   macro avg       0.85      0.84      0.84      5396
weighted avg       0.83      0.83      0.83      5396

2025-01-23 16:51:40,369 - INFO - numLayer1_hiddenSize64_seq_num100_gap3_accuracy90.18, total samples: 5374
2025-01-23 16:51:40,370 - INFO - 
 [[790  38  53   0   0   0   0]
 [ 50 796 211   0   0   0   0]
 [ 74  96 728   0   0  21   0]
 [  0   0   7 648   0   7   0]
 [  0   0   1   0 545   0   0]
 [ 16  16   0  33   0 678   0]
 [ 10   0   0   0   7  97 452]]
2025-01-23 16:51:40,376 - INFO - 
               precision    recall  f1-score   support

           1       0.84      0.90      0.87       881
           2       0.84      0.75      0.79      1057
           3       0.73      0.79      0.76       919
           4       0.95      0.98      0.97       662
           5       0.99      1.00      0.99       546
           6       0.84      0.91      0.88       743
           7       1.00      0.80      0.89       566

    accuracy                           0.86      5374
   macro avg       0.88      0.88      0.88      5374
weighted avg       0.87      0.86      0.86      5374

2025-01-23 16:51:40,381 - INFO - numLayer1_hiddenSize64_seq_num50_gap5_accuracy87.95, total samples: 5448
2025-01-23 16:51:40,381 - INFO - 
 [[759  49  34  43   2   2   9]
 [ 54 751 211  49   0  11   0]
 [ 71 112 725   0   0   0   0]
 [ 26  17  80 529   0   0   9]
 [  0   0   0   0 569   0   0]
 [ 16   0   0   0   4 740   0]
 [  3   3   0   0   0 124 446]]
2025-01-23 16:51:40,388 - INFO - 
               precision    recall  f1-score   support

           1       0.82      0.85      0.83       898
           2       0.81      0.70      0.75      1076
           3       0.69      0.80      0.74       908
           4       0.85      0.80      0.83       661
           5       0.99      1.00      0.99       569
           6       0.84      0.97      0.90       760
           7       0.96      0.77      0.86       576

    accuracy                           0.83      5448
   macro avg       0.85      0.84      0.84      5448
weighted avg       0.83      0.83      0.83      5448

2025-01-23 16:51:40,393 - INFO - numLayer1_hiddenSize64_seq_num80_gap5_accuracy88.35, total samples: 5408
2025-01-23 16:51:40,394 - INFO - 
 [[760  55  42  10  12   0  12]
 [111 710 157  51  22   0  13]
 [ 78 133 688  24   0   0   0]
 [  0  30  61 547  11   4   8]
 [  0   0   0   0 548   0  10]
 [  0  21   2  16   2 709   0]
 [ 17   0   0   3   6 126 409]]
2025-01-23 16:51:40,401 - INFO - 
               precision    recall  f1-score   support

           1       0.79      0.85      0.82       891
           2       0.75      0.67      0.71      1064
           3       0.72      0.75      0.73       923
           4       0.84      0.83      0.83       661
           5       0.91      0.98      0.95       558
           6       0.85      0.95      0.89       750
           7       0.90      0.73      0.81       561

    accuracy                           0.81      5408
   macro avg       0.82      0.82      0.82      5408
weighted avg       0.81      0.81      0.81      5408

2025-01-23 16:51:40,405 - INFO - numLayer1_hiddenSize64_seq_num100_gap5_accuracy90.87, total samples: 5375
2025-01-23 16:51:40,406 - INFO - 
 [[801  35  36   0   9   0   0]
 [ 65 791 158  28  13   0   0]
 [  9  85 741  60  18   0   7]
 [  0   0  23 642   0   0   0]
 [  0   0   0  14 534   0   0]
 [ 24  15   0   5   0 698   0]
 [  0   0   0   1   0 101 462]]
2025-01-23 16:51:40,413 - INFO - 
               precision    recall  f1-score   support

           1       0.89      0.91      0.90       881
           2       0.85      0.75      0.80      1055
           3       0.77      0.81      0.79       920
           4       0.86      0.97      0.91       665
           5       0.93      0.97      0.95       548
           6       0.87      0.94      0.91       742
           7       0.99      0.82      0.89       564

    accuracy                           0.87      5375
   macro avg       0.88      0.88      0.88      5375
weighted avg       0.87      0.87      0.87      5375

2025-01-23 16:51:40,418 - INFO - numLayer1_hiddenSize128_seq_num50_gap3_accuracy88.68, total samples: 5465
2025-01-23 16:51:40,418 - INFO - 
 [[775  47  50   2  12   8   3]
 [109 722 177  44  18   0  10]
 [ 71 140 701   1   0   0   0]
 [ 20   8  51 523  21  16  21]
 [  0   0   0  24 546   0   0]
 [ 10   0   6  14   5 726   4]
 [ 15   0   0   8   0 139 418]]
2025-01-23 16:51:40,425 - INFO - 
               precision    recall  f1-score   support

           1       0.78      0.86      0.82       897
           2       0.79      0.67      0.72      1080
           3       0.71      0.77      0.74       913
           4       0.85      0.79      0.82       660
           5       0.91      0.96      0.93       570
           6       0.82      0.95      0.88       765
           7       0.92      0.72      0.81       580

    accuracy                           0.81      5465
   macro avg       0.82      0.82      0.82      5465
weighted avg       0.81      0.81      0.81      5465

2025-01-23 16:51:40,430 - INFO - numLayer1_hiddenSize128_seq_num80_gap3_accuracy88.40, total samples: 5409
2025-01-23 16:51:40,430 - INFO - 
 [[761  58  50   7  14   0   0]
 [114 677 194  72  10   0   0]
 [ 77 101 737   8   0   0   0]
 [  0   0  27 593  27  13   0]
 [  0   0   0   0 556   0   0]
 [  0  30  13  10   0 698   0]
 [ 18   0   0   0   0 105 439]]
2025-01-23 16:51:40,437 - INFO - 
               precision    recall  f1-score   support

           1       0.78      0.86      0.82       890
           2       0.78      0.63      0.70      1067
           3       0.72      0.80      0.76       923
           4       0.86      0.90      0.88       660
           5       0.92      1.00      0.96       556
           6       0.86      0.93      0.89       751
           7       1.00      0.78      0.88       562

    accuracy                           0.82      5409
   macro avg       0.85      0.84      0.84      5409
weighted avg       0.83      0.82      0.82      5409

2025-01-23 16:51:40,442 - INFO - numLayer1_hiddenSize128_seq_num100_gap3_accuracy91.72, total samples: 5380
2025-01-23 16:51:40,443 - INFO - 
 [[819   8  56   0   0   0   0]
 [ 97 729 214  17   0   0   0]
 [ 44 111 729  12   0   6  16]
 [  0   0   0 661   0   4   0]
 [  0   0   0   0 548   0   0]
 [ 18   0  13  22   0 690   0]
 [ 12   0   0   0   4 103 447]]
2025-01-23 16:51:40,450 - INFO - 
               precision    recall  f1-score   support

           1       0.83      0.93      0.87       883
           2       0.86      0.69      0.77      1057
           3       0.72      0.79      0.76       918
           4       0.93      0.99      0.96       665
           5       0.99      1.00      1.00       548
           6       0.86      0.93      0.89       743
           7       0.97      0.79      0.87       566

    accuracy                           0.86      5380
   macro avg       0.88      0.87      0.87      5380
weighted avg       0.86      0.86      0.86      5380

2025-01-23 16:51:40,454 - INFO - numLayer1_hiddenSize128_seq_num50_gap5_accuracy89.37, total samples: 5459
2025-01-23 16:51:40,455 - INFO - 
 [[789  25  64  15   7   0   0]
 [126 734 131  74   0   1   7]
 [ 67  93 742   8   0   0   1]
 [ 21  48  55 537   0   1   0]
 [  0   0  11   0 547   4   7]
 [ 19   0   0  11   0 726  10]
 [  0   0   0   0  11 105 462]]
2025-01-23 16:51:40,461 - INFO - 
               precision    recall  f1-score   support

           1       0.77      0.88      0.82       900
           2       0.82      0.68      0.74      1073
           3       0.74      0.81      0.78       911
           4       0.83      0.81      0.82       662
           5       0.97      0.96      0.96       569
           6       0.87      0.95      0.91       766
           7       0.95      0.80      0.87       578

    accuracy                           0.83      5459
   macro avg       0.85      0.84      0.84      5459
weighted avg       0.84      0.83      0.83      5459

2025-01-23 16:51:40,466 - INFO - numLayer1_hiddenSize128_seq_num80_gap5_accuracy90.97, total samples: 5396
2025-01-23 16:51:40,467 - INFO - 
 [[814  28  45   0   0   0   2]
 [ 99 692 164  97  12   0   0]
 [ 97 134 684   0   0   3   2]
 [  0   4  58 494   0 103   0]
 [  0   0   0   1 540  10   0]
 [ 10   5   6  14   0 715   0]
 [ 15   0   0   0   2 109 437]]
2025-01-23 16:51:40,473 - INFO - 
               precision    recall  f1-score   support

           1       0.79      0.92      0.85       889
           2       0.80      0.65      0.72      1064
           3       0.71      0.74      0.73       920
           4       0.82      0.75      0.78       659
           5       0.97      0.98      0.98       551
           6       0.76      0.95      0.85       750
           7       0.99      0.78      0.87       563

    accuracy                           0.81      5396
   macro avg       0.83      0.82      0.82      5396
weighted avg       0.82      0.81      0.81      5396

2025-01-23 16:51:40,478 - INFO - numLayer1_hiddenSize128_seq_num100_gap5_accuracy92.36, total samples: 5379
2025-01-23 16:51:40,479 - INFO - 
 [[831   4  46   0   0   0   0]
 [112 758 190   0   0   0   0]
 [ 46 100 753   0  22   0   0]
 [  0   1   0 661   0   1   0]
 [  0   0   0   0 548   0   0]
 [ 22  12   0  19   1 686   0]
 [  0   0   0   0  10 115 441]]
2025-01-23 16:51:40,485 - INFO - 
               precision    recall  f1-score   support

           1       0.82      0.94      0.88       881
           2       0.87      0.72      0.78      1060
           3       0.76      0.82      0.79       921
           4       0.97      1.00      0.98       663
           5       0.94      1.00      0.97       548
           6       0.86      0.93      0.89       740
           7       1.00      0.78      0.88       566

    accuracy                           0.87      5379
   macro avg       0.89      0.88      0.88      5379
weighted avg       0.87      0.87      0.87      5379

2025-01-29 17:57:54,021 - INFO - trained model on ur5/trained_models/contact_localization/62/, tested on ur5, with 6 links
2025-01-29 18:00:18,727 - INFO - numLayer1_hiddenSize64_seq_num80_gap3_accuracy83.95, total samples: 4436
2025-01-29 18:00:18,728 - INFO - 
 [[309 121  73  19  34  16]
 [ 84 620   0  17   0   0]
 [ 12  94 691  66 117  30]
 [ 45 164  26 409  21   0]
 [  2   5 117  67 471   8]
 [200 142  42   9   0 405]]
2025-01-29 18:00:18,734 - INFO - 
               precision    recall  f1-score   support

           1       0.47      0.54      0.50       572
           2       0.54      0.86      0.66       721
           3       0.73      0.68      0.71      1010
           4       0.70      0.62      0.65       665
           5       0.73      0.70      0.72       670
           6       0.88      0.51      0.64       798

    accuracy                           0.65      4436
   macro avg       0.68      0.65      0.65      4436
weighted avg       0.69      0.65      0.66      4436

2025-01-29 18:00:18,738 - INFO - numLayer1_hiddenSize64_seq_num100_gap3_accuracy86.76, total samples: 4441
2025-01-29 18:00:18,739 - INFO - 
 [[372  74  50   3  72   0]
 [ 67 626  14   5   0   0]
 [  0  76 788  72  78   0]
 [ 76  99  26 447  22   0]
 [  0   0 110  15 548   0]
 [182 259  66  50  35 209]]
2025-01-29 18:00:18,745 - INFO - 
               precision    recall  f1-score   support

           1       0.53      0.65      0.59       571
           2       0.55      0.88      0.68       712
           3       0.75      0.78      0.76      1014
           4       0.76      0.67      0.71       670
           5       0.73      0.81      0.77       673
           6       1.00      0.26      0.41       801

    accuracy                           0.67      4441
   macro avg       0.72      0.68      0.65      4441
weighted avg       0.73      0.67      0.66      4441

2025-01-29 18:00:18,749 - INFO - numLayer1_hiddenSize64_seq_num80_gap5_accuracy85.45, total samples: 4430
2025-01-29 18:00:18,749 - INFO - 
 [[338 101  82   0  34  19]
 [107 543  27  19   0  27]
 [ 18  52 629  92 189  27]
 [ 66 126  21 434  13   0]
 [  0   7  90  26 536   9]
 [217 328   1  62   0 190]]
2025-01-29 18:00:18,756 - INFO - 
               precision    recall  f1-score   support

           1       0.45      0.59      0.51       574
           2       0.47      0.75      0.58       723
           3       0.74      0.62      0.68      1007
           4       0.69      0.66      0.67       660
           5       0.69      0.80      0.74       668
           6       0.70      0.24      0.36       798

    accuracy                           0.60      4430
   macro avg       0.62      0.61      0.59      4430
weighted avg       0.64      0.60      0.59      4430

2025-01-29 18:00:18,760 - INFO - numLayer1_hiddenSize64_seq_num100_gap5_accuracy86.84, total samples: 4438
2025-01-29 18:00:18,761 - INFO - 
 [[376  99  24  25  49   0]
 [113 534   0  65   0   0]
 [ 20  52 691 126 110  11]
 [ 75  69  47 388  89   0]
 [  0   0  33  15 626   0]
 [173 222  34  43  44 285]]
2025-01-29 18:00:18,767 - INFO - 
               precision    recall  f1-score   support

           1       0.50      0.66      0.57       573
           2       0.55      0.75      0.63       712
           3       0.83      0.68      0.75      1010
           4       0.59      0.58      0.58       668
           5       0.68      0.93      0.79       674
           6       0.96      0.36      0.52       801

    accuracy                           0.65      4438
   macro avg       0.68      0.66      0.64      4438
weighted avg       0.71      0.65      0.65      4438

2025-01-29 18:00:18,771 - INFO - numLayer1_hiddenSize128_seq_num80_gap3_accuracy87.20, total samples: 4439
2025-01-29 18:00:18,771 - INFO - 
 [[384  54  69   0  45  22]
 [101 587   0   2   0  33]
 [ 17  69 678  99 125  21]
 [ 65 149  24 386  32  10]
 [  0  20 105  50 477  17]
 [189 138   0   0   0 471]]
2025-01-29 18:00:18,777 - INFO - 
               precision    recall  f1-score   support

           1       0.51      0.67      0.58       574
           2       0.58      0.81      0.67       723
           3       0.77      0.67      0.72      1009
           4       0.72      0.58      0.64       666
           5       0.70      0.71      0.71       669
           6       0.82      0.59      0.69       798

    accuracy                           0.67      4439
   macro avg       0.68      0.67      0.67      4439
weighted avg       0.70      0.67      0.67      4439

2025-01-29 18:00:18,781 - INFO - numLayer1_hiddenSize128_seq_num100_gap3_accuracy87.06, total samples: 4438
2025-01-29 18:00:18,782 - INFO - 
 [[384  74  47   0  69   0]
 [ 89 599   8  17   0   0]
 [  8  73 698  89 147   0]
 [ 62 100  30 445  19  11]
 [  0   0  60   8 605   0]
 [226 198  28  24   0 320]]
2025-01-29 18:00:18,788 - INFO - 
               precision    recall  f1-score   support

           1       0.50      0.67      0.57       574
           2       0.57      0.84      0.68       713
           3       0.80      0.69      0.74      1015
           4       0.76      0.67      0.71       667
           5       0.72      0.90      0.80       673
           6       0.97      0.40      0.57       796

    accuracy                           0.69      4438
   macro avg       0.72      0.69      0.68      4438
weighted avg       0.74      0.69      0.68      4438

2025-01-29 18:00:18,792 - INFO - numLayer1_hiddenSize128_seq_num80_gap5_accuracy85.70, total samples: 4433
2025-01-29 18:00:18,793 - INFO - 
 [[349 120  71   0   7  26]
 [112 599   0  13   0   0]
 [ 42  56 696  63 123  26]
 [ 71 135  31 382  45   0]
 [ 10   9  91  59 498   0]
 [228 210  41  23   0 297]]
2025-01-29 18:00:18,799 - INFO - 
               precision    recall  f1-score   support

           1       0.43      0.61      0.50       573
           2       0.53      0.83      0.65       724
           3       0.75      0.69      0.72      1006
           4       0.71      0.58      0.63       664
           5       0.74      0.75      0.74       667
           6       0.85      0.37      0.52       799

    accuracy                           0.64      4433
   macro avg       0.67      0.64      0.63      4433
weighted avg       0.68      0.64      0.63      4433

2025-01-29 18:00:18,802 - INFO - numLayer1_hiddenSize128_seq_num100_gap5_accuracy86.88, total samples: 4424
2025-01-29 18:00:18,803 - INFO - 
 [[379  87  36   0  75   0]
 [ 87 613   6   0   0   4]
 [  0  89 710 110 104   0]
 [ 62 139  25 399  30  10]
 [  0   0  99  15 541  12]
 [187 305   0   1   9 290]]
2025-01-29 18:00:18,809 - INFO - 
               precision    recall  f1-score   support

           1       0.53      0.66      0.59       577
           2       0.50      0.86      0.63       710
           3       0.81      0.70      0.75      1013
           4       0.76      0.60      0.67       665
           5       0.71      0.81      0.76       667
           6       0.92      0.37      0.52       792

    accuracy                           0.66      4424
   macro avg       0.70      0.67      0.65      4424
weighted avg       0.72      0.66      0.66      4424

---------------------------------------------------------------------------------------------------------------------------------------
2025-01-29 17:49:29,197  - INFO - trained model on franka_main/trained_models/contact_localization/62/, tested on ur5, with 6 links

2025-01-29 17:49:29,197 - INFO - numLayer1_hiddenSize64_seq_num80_gap3_accuracy91.68, total samples: 4414
2025-01-29 17:49:29,198 - INFO - 
 [[ 20 206 192  51  69  31]
 [ 18 453  46 112  56  31]
 [ 10  95 102 686  59  55]
 [ 31  97  63 387  36  47]
 [  0   9  30 463  93  71]
 [ 10 390  36 208  91  60]]
2025-01-29 17:49:29,205 - INFO - 
               precision    recall  f1-score   support

           1       0.22      0.04      0.06       569
           2       0.36      0.63      0.46       716
           3       0.22      0.10      0.14      1007
           4       0.20      0.59      0.30       661
           5       0.23      0.14      0.17       666
           6       0.20      0.08      0.11       795

    accuracy                           0.25      4414
   macro avg       0.24      0.26      0.21      4414
weighted avg       0.24      0.25      0.21      4414

2025-01-29 17:49:29,209 - INFO - numLayer1_hiddenSize64_seq_num100_gap3_accuracy80.91, total samples: 4425
2025-01-29 17:49:29,210 - INFO - 
 [[147 173 184  26  18  24]
 [ 27 350 113 134  73  12]
 [ 14  86  92 743  65  13]
 [ 24  82 136 345  15  66]
 [  0   0  22 546  78  20]
 [ 64 301  53 201 115  63]]
2025-01-29 17:49:29,216 - INFO - 
               precision    recall  f1-score   support

           1       0.53      0.26      0.35       572
           2       0.35      0.49      0.41       709
           3       0.15      0.09      0.11      1013
           4       0.17      0.52      0.26       668
           5       0.21      0.12      0.15       666
           6       0.32      0.08      0.13       797

    accuracy                           0.24      4425
   macro avg       0.29      0.26      0.23      4425
weighted avg       0.28      0.24      0.22      4425

2025-01-29 17:49:29,220 - INFO - numLayer1_hiddenSize64_seq_num80_gap5_accuracy91.13, total samples: 4425
2025-01-29 17:49:29,221 - INFO - 
 [[121 177 110  53   3 106]
 [ 27 415  66  84  71  55]
 [ 21  81 232 417 135 124]
 [ 16  78 217  99 156 101]
 [  5   3 180 301  80  96]
 [ 13 231 110 206 115 120]]
2025-01-29 17:49:29,227 - INFO - 
               precision    recall  f1-score   support

           1       0.60      0.21      0.31       570
           2       0.42      0.58      0.49       718
           3       0.25      0.23      0.24      1010
           4       0.09      0.15      0.11       667
           5       0.14      0.12      0.13       665
           6       0.20      0.15      0.17       795

    accuracy                           0.24      4425
   macro avg       0.28      0.24      0.24      4425
weighted avg       0.27      0.24      0.24      4425

2025-01-29 17:49:29,231 - INFO - numLayer1_hiddenSize64_seq_num100_gap5_accuracy91.85, total samples: 4424
2025-01-29 17:49:29,232 - INFO - 
 [[ 41 229 187  63  30  23]
 [  0 459  13 164  74   0]
 [  0  69  80 785  76   4]
 [ 24  60  85 446  20  29]
 [  0   1  22 572  53  19]
 [ 24 305  73 280  57  57]]
2025-01-29 17:49:29,238 - INFO - 
               precision    recall  f1-score   support

           1       0.46      0.07      0.12       573
           2       0.41      0.65      0.50       710
           3       0.17      0.08      0.11      1014
           4       0.19      0.67      0.30       664
           5       0.17      0.08      0.11       667
           6       0.43      0.07      0.12       796

    accuracy                           0.26      4424
   macro avg       0.31      0.27      0.21      4424
weighted avg       0.30      0.26      0.20      4424

2025-01-29 17:49:29,242 - INFO - numLayer1_hiddenSize128_seq_num80_gap3_accuracy91.60, total samples: 4425
2025-01-29 17:49:29,243 - INFO - 
 [[ 34 215 164 113  20  24]
 [  0 358 104 136 113   7]
 [  0  81  62 771  39  60]
 [  0  47 194 350  14  58]
 [  0  11  10 582   0  63]
 [ 13 276 123 225  87  71]]
2025-01-29 17:49:29,249 - INFO - 
               precision    recall  f1-score   support

           1       0.72      0.06      0.11       570
           2       0.36      0.50      0.42       718
           3       0.09      0.06      0.07      1013
           4       0.16      0.53      0.25       663
           5       0.00      0.00      0.00       666
           6       0.25      0.09      0.13       795

    accuracy                           0.20      4425
   macro avg       0.27      0.21      0.16      4425
weighted avg       0.24      0.20      0.16      4425

2025-01-29 17:49:29,253 - INFO - numLayer1_hiddenSize128_seq_num100_gap3_accuracy91.61, total samples: 4424
2025-01-29 17:49:29,254 - INFO - 
 [[ 84 208 168  88  23   0]
 [  0 544  40 113  15   0]
 [ 11  98 116 755  29   0]
 [ 33  88 173 337  25  10]
 [  0  10  15 591  47   8]
 [ 28 425 104 170  68   0]]
2025-01-29 17:49:29,260 - INFO - 
               precision    recall  f1-score   support

           1       0.54      0.15      0.23       571
           2       0.40      0.76      0.52       712
           3       0.19      0.11      0.14      1009
           4       0.16      0.51      0.25       666
           5       0.23      0.07      0.11       671
           6       0.00      0.00      0.00       795

    accuracy                           0.25      4424
   macro avg       0.25      0.27      0.21      4424
weighted avg       0.24      0.25      0.20      4424

2025-01-29 17:49:29,264 - INFO - numLayer1_hiddenSize128_seq_num80_gap5_accuracy91.52, total samples: 4435
2025-01-29 17:49:29,265 - INFO - 
 [[ 45 137 273  62  35  20]
 [ 19 367  33 234  66   0]
 [ 10  92 152 633  61  62]
 [  9  31 196 346  55  31]
 [  0   0  17 535  61  59]
 [ 15 294  69 299  80  37]]
2025-01-29 17:49:29,271 - INFO - 
               precision    recall  f1-score   support

           1       0.46      0.08      0.13       572
           2       0.40      0.51      0.45       719
           3       0.21      0.15      0.17      1010
           4       0.16      0.52      0.25       668
           5       0.17      0.09      0.12       672
           6       0.18      0.05      0.07       794

    accuracy                           0.23      4435
   macro avg       0.26      0.23      0.20      4435
weighted avg       0.25      0.23      0.20      4435

2025-01-29 17:49:29,275 - INFO - numLayer1_hiddenSize128_seq_num100_gap5_accuracy91.61, total samples: 4424
2025-01-29 17:49:29,276 - INFO - 
 [[ 73 252 155  77   8   7]
 [102 322  12 207  68   0]
 [ 20 136  35 794  20   5]
 [  1  93 114 457   0   0]
 [  0  16   0 611  43   1]
 [ 27 258  51 292 141  26]]
2025-01-29 17:49:29,282 - INFO - 
               precision    recall  f1-score   support

           1       0.33      0.13      0.18       572
           2       0.30      0.45      0.36       711
           3       0.10      0.03      0.05      1010
           4       0.19      0.69      0.29       665
           5       0.15      0.06      0.09       671
           6       0.67      0.03      0.06       795

    accuracy                           0.22      4424
   macro avg       0.29      0.23      0.17      4424
weighted avg       0.28      0.22      0.16      4424
----------------------------------------------------------------------------------------------------------------------------
v1- lr = 0.0005

2025-01-29 18:09:03,667 - INFO - fine-tuning franka_main/trained_models/contact_localization/62/ on ur5 with 6 links
2025-01-29 18:09:03,737 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=64 seq_num = 80, gap = 3, --------------
2025-01-29 18:09:11,326 - INFO - Accuracy on the test data = [85.61673164367676]
2025-01-29 18:09:11,378 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=64 seq_num = 100, gap = 3, --------------
2025-01-29 18:09:18,225 - INFO - Accuracy on the test data = [84.69781279563904]
2025-01-29 18:09:18,285 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=64 seq_num = 80, gap = 5, --------------
2025-01-29 18:09:24,602 - INFO - Accuracy on the test data = [83.31199288368225]
2025-01-29 18:09:24,645 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=64 seq_num = 100, gap = 5, --------------
2025-01-29 18:09:30,583 - INFO - Accuracy on the test data = [84.74067449569702]
2025-01-29 18:09:30,661 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=128 seq_num = 80, gap = 3, --------------
2025-01-29 18:09:37,462 - INFO - Accuracy on the test data = [84.89116430282593]
2025-01-29 18:09:37,516 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=128 seq_num = 100, gap = 3, --------------
2025-01-29 18:09:43,681 - INFO - Accuracy on the test data = [84.86926555633545]
2025-01-29 18:09:43,747 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=128 seq_num = 80, gap = 5, --------------
2025-01-29 18:09:50,147 - INFO - Accuracy on the test data = [82.54374861717224]
2025-01-29 18:09:50,193 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=128 seq_num = 100, gap = 5, --------------
2025-01-29 18:09:55,781 - INFO - Accuracy on the test data = [81.26875162124634]
2025-01-29 18:12:41,965 - INFO - Fine-tuned source model with ur5, tested on ur5, with 6 links
2025-01-29 18:14:58,346 - INFO - numLayer1_hiddenSize64_seq_num80_gap3_accuracy85.62, total samples: 4430
2025-01-29 18:14:58,347 - INFO - 
 [[346 186  42   0   0   0]
 [116 604   1   0   0   0]
 [ 31  86 705 100  65  20]
 [ 26 232  20 358   0  29]
 [ 14   6  54 136 443  10]
 [258 340   0   0   0 202]]
2025-01-29 18:14:58,353 - INFO - 
               precision    recall  f1-score   support

           1       0.44      0.60      0.51       574
           2       0.42      0.84      0.56       721
           3       0.86      0.70      0.77      1007
           4       0.60      0.54      0.57       665
           5       0.87      0.67      0.76       663
           6       0.77      0.25      0.38       800

    accuracy                           0.60      4430
   macro avg       0.66      0.60      0.59      4430
weighted avg       0.68      0.60      0.60      4430

2025-01-29 18:14:58,358 - INFO - numLayer1_hiddenSize64_seq_num100_gap3_accuracy84.70, total samples: 4434
2025-01-29 18:14:58,359 - INFO - 
 [[327 203  46   0   0   0]
 [ 62 600  38   1   0  12]
 [  0  72 814  71  50   1]
 [ 51 147  32 395  31   9]
 [  0   0  88  31 554   0]
 [160 434  39   1   0 165]]
2025-01-29 18:14:58,365 - INFO - 
               precision    recall  f1-score   support

           1       0.55      0.57      0.56       576
           2       0.41      0.84      0.55       713
           3       0.77      0.81      0.79      1008
           4       0.79      0.59      0.68       665
           5       0.87      0.82      0.85       673
           6       0.88      0.21      0.33       799

    accuracy                           0.64      4434
   macro avg       0.71      0.64      0.63      4434
weighted avg       0.72      0.64      0.63      4434

2025-01-29 18:14:58,369 - INFO - numLayer1_hiddenSize64_seq_num80_gap5_accuracy83.31, total samples: 4426
2025-01-29 18:14:58,369 - INFO - 
 [[292 261  21   0   0   0]
 [107 580   9  15   0  13]
 [ 50  42 714 131  47  21]
 [ 31 213  34 354   0  29]
 [  4  15  93 154 372  26]
 [219 342  20   5  11 201]]
2025-01-29 18:14:58,376 - INFO - 
               precision    recall  f1-score   support

           1       0.42      0.51      0.46       574
           2       0.40      0.80      0.53       724
           3       0.80      0.71      0.75      1005
           4       0.54      0.54      0.54       661
           5       0.87      0.56      0.68       664
           6       0.69      0.25      0.37       798

    accuracy                           0.57      4426
   macro avg       0.62      0.56      0.55      4426
weighted avg       0.64      0.57      0.57      4426

2025-01-29 18:14:58,380 - INFO - numLayer1_hiddenSize64_seq_num100_gap5_accuracy84.74, total samples: 4429
2025-01-29 18:14:58,380 - INFO - 
 [[329 204  42   0   0   0]
 [ 91 619   0   0   0   0]
 [ 25  41 670 133 133   9]
 [ 48 160  24 357  41  34]
 [  0   9  17  97 533  14]
 [212 351  17   1   0 218]]
2025-01-29 18:14:58,386 - INFO - 
               precision    recall  f1-score   support

           1       0.47      0.57      0.51       575
           2       0.45      0.87      0.59       710
           3       0.87      0.66      0.75      1011
           4       0.61      0.54      0.57       664
           5       0.75      0.80      0.77       670
           6       0.79      0.27      0.41       799

    accuracy                           0.62      4429
   macro avg       0.66      0.62      0.60      4429
weighted avg       0.68      0.62      0.61      4429

2025-01-29 18:14:58,391 - INFO - numLayer1_hiddenSize128_seq_num80_gap3_accuracy84.89, total samples: 4429
2025-01-29 18:14:58,391 - INFO - 
 [[329 218  16   0  13   0]
 [109 597   0   0   0  15]
 [ 49  33 595 154 151  25]
 [ 75 185  24 380   0   0]
 [  0   2  87 181 382  12]
 [252 295   0   0  16 234]]
2025-01-29 18:14:58,398 - INFO - 
               precision    recall  f1-score   support

           1       0.40      0.57      0.47       576
           2       0.45      0.83      0.58       721
           3       0.82      0.59      0.69      1007
           4       0.53      0.57      0.55       664
           5       0.68      0.58      0.62       664
           6       0.82      0.29      0.43       797

    accuracy                           0.57      4429
   macro avg       0.62      0.57      0.56      4429
weighted avg       0.64      0.57      0.57      4429

2025-01-29 18:14:58,402 - INFO - numLayer1_hiddenSize128_seq_num100_gap3_accuracy84.87, total samples: 4439
2025-01-29 18:14:58,403 - INFO - 
 [[333 135 108   0   0   0]
 [ 47 659   5   0   0   0]
 [ 27  15 744  62 166   0]
 [ 56 171  81 338   0  24]
 [  0  10  53  44 564   0]
 [173 338  27   6  10 243]]
2025-01-29 18:14:58,409 - INFO - 
               precision    recall  f1-score   support

           1       0.52      0.58      0.55       576
           2       0.50      0.93      0.65       711
           3       0.73      0.73      0.73      1014
           4       0.75      0.50      0.60       670
           5       0.76      0.84      0.80       671
           6       0.91      0.30      0.46       797

    accuracy                           0.65      4439
   macro avg       0.70      0.65      0.63      4439
weighted avg       0.71      0.65      0.64      4439

2025-01-29 18:14:58,413 - INFO - numLayer1_hiddenSize128_seq_num80_gap5_accuracy82.54, total samples: 4434
2025-01-29 18:14:58,414 - INFO - 
 [[275 212  78   0   0   8]
 [117 589  17   0   0   0]
 [ 23  87 689 128  57  25]
 [ 58 180  58 355   2  15]
 [  0   5  68 191 394   6]
 [258 347  17  24   0 151]]
2025-01-29 18:14:58,420 - INFO - 
               precision    recall  f1-score   support

           1       0.38      0.48      0.42       573
           2       0.41      0.81      0.55       723
           3       0.74      0.68      0.71      1009
           4       0.51      0.53      0.52       668
           5       0.87      0.59      0.71       664
           6       0.74      0.19      0.30       797

    accuracy                           0.55      4434
   macro avg       0.61      0.55      0.53      4434
weighted avg       0.62      0.55      0.54      4434

2025-01-29 18:14:58,424 - INFO - numLayer1_hiddenSize128_seq_num100_gap5_accuracy81.27, total samples: 4433
2025-01-29 18:14:58,425 - INFO - 
 [[245 286  42   0   0   0]
 [ 92 592  21   5   0   0]
 [ 36  62 633 154 114  11]
 [ 51 131  67 404   3  13]
 [  0   3   6  72 589   0]
 [260 292  18   0   0 231]]
2025-01-29 18:14:58,431 - INFO - 
               precision    recall  f1-score   support

           1       0.36      0.43      0.39       573
           2       0.43      0.83      0.57       710
           3       0.80      0.63      0.70      1010
           4       0.64      0.60      0.62       669
           5       0.83      0.88      0.86       670
           6       0.91      0.29      0.44       801

    accuracy                           0.61      4433
   macro avg       0.66      0.61      0.60      4433
weighted avg       0.68      0.61      0.60      4433

--------------------------------------------------------------------------------------------------------------------------
v2- lr = 0.004

2025-01-29 20:35:00,191 - INFO - fine-tuning franka_main/trained_models/contact_localization/62/ on ur5 with 6 links
2025-01-29 20:35:00,279 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=64 seq_num = 80, gap = 3, --------------
2025-01-29 20:35:05,931 - INFO - Accuracy on the test data = [85.78745126724243]
2025-01-29 20:35:05,989 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=64 seq_num = 100, gap = 3, --------------
2025-01-29 20:35:10,711 - INFO - Accuracy on the test data = [83.28332901000977]
2025-01-29 20:35:10,780 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=64 seq_num = 80, gap = 5, --------------
2025-01-29 20:35:15,417 - INFO - Accuracy on the test data = [89.67136144638062]
2025-01-29 20:35:15,465 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=64 seq_num = 100, gap = 5, --------------
2025-01-29 20:35:19,693 - INFO - Accuracy on the test data = [86.62666082382202]
2025-01-29 20:35:19,778 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=128 seq_num = 80, gap = 3, --------------
2025-01-29 20:35:25,174 - INFO - Accuracy on the test data = [88.09219002723694]
2025-01-29 20:35:25,235 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=128 seq_num = 100, gap = 3, --------------
2025-01-29 20:35:29,883 - INFO - Accuracy on the test data = [88.7698233127594]
2025-01-29 20:35:29,959 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=128 seq_num = 80, gap = 5, --------------
2025-01-29 20:35:34,660 - INFO - Accuracy on the test data = [85.91549396514893]
2025-01-29 20:35:34,712 - INFO - ------------  model_cnnLSTM , num_layers = 1, hidden_size=128 seq_num = 100, gap = 5, --------------
2025-01-29 20:35:38,928 - INFO - Accuracy on the test data = [85.72653532028198]
2025-01-29 20:35:42,402 - INFO - Fine-tuned source model with ur5, tested on ur5, with 6 links
2025-01-29 20:38:04,910 - INFO - numLayer1_hiddenSize64_seq_num80_gap3_accuracy85.79, total samples: 4427
2025-01-29 20:38:04,911 - INFO - 
 [[352 120  59  16  10  12]
 [ 91 604   8  12   8   0]
 [ 48  34 766  85  72   0]
 [ 55 202  46 346   0  19]
 [  3   6  71 180 392  14]
 [158 323  28  20   7 260]]
2025-01-29 20:38:04,917 - INFO - 
               precision    recall  f1-score   support

           1       0.50      0.62      0.55       569
           2       0.47      0.84      0.60       723
           3       0.78      0.76      0.77      1005
           4       0.53      0.52      0.52       668
           5       0.80      0.59      0.68       666
           6       0.85      0.33      0.47       796

    accuracy                           0.61      4427
   macro avg       0.65      0.61      0.60      4427
weighted avg       0.67      0.61      0.61      4427

2025-01-29 20:38:04,922 - INFO - numLayer1_hiddenSize64_seq_num100_gap3_accuracy83.28, total samples: 4427
2025-01-29 20:38:04,922 - INFO - 
 [[297 181  80   0   1  15]
 [ 52 660   0   0   0   0]
 [ 10  53 830  54  63   0]
 [ 56 113  50 410  17  16]
 [  0   1  60  46 563   0]
 [188 331  24  19   0 237]]
2025-01-29 20:38:04,928 - INFO - 
               precision    recall  f1-score   support

           1       0.49      0.52      0.50       574
           2       0.49      0.93      0.64       712
           3       0.80      0.82      0.81      1010
           4       0.78      0.62      0.69       662
           5       0.87      0.84      0.86       670
           6       0.88      0.30      0.44       799

    accuracy                           0.68      4427
   macro avg       0.72      0.67      0.66      4427
weighted avg       0.73      0.68      0.67      4427

2025-01-29 20:38:04,932 - INFO - numLayer1_hiddenSize64_seq_num80_gap5_accuracy89.67, total samples: 4435
2025-01-29 20:38:04,933 - INFO - 
 [[440  69  47  11   5   0]
 [ 83 581   9  16  14  20]
 [ 42  33 613 184 115  24]
 [ 72 159  22 398   5  12]
 [  9   0  41 163 420  29]
 [161 268   0   0   0 370]]
2025-01-29 20:38:04,939 - INFO - 
               precision    recall  f1-score   support

           1       0.55      0.77      0.64       572
           2       0.52      0.80      0.63       723
           3       0.84      0.61      0.70      1011
           4       0.52      0.60      0.55       668
           5       0.75      0.63      0.69       662
           6       0.81      0.46      0.59       799

    accuracy                           0.64      4435
   macro avg       0.66      0.65      0.63      4435
weighted avg       0.68      0.64      0.64      4435

2025-01-29 20:38:04,943 - INFO - numLayer1_hiddenSize64_seq_num100_gap5_accuracy86.63, total samples: 4436
2025-01-29 20:38:04,944 - INFO - 
 [[375 159  38   0   0   0]
 [ 84 628   0   0   0   0]
 [ 13  96 749  76  82   0]
 [ 40 139  62 401   4  21]
 [  0   0  50 127 494   0]
 [213 359  25  21   0 180]]
2025-01-29 20:38:04,949 - INFO - 
               precision    recall  f1-score   support

           1       0.52      0.66      0.58       572
           2       0.45      0.88      0.60       712
           3       0.81      0.74      0.77      1016
           4       0.64      0.60      0.62       667
           5       0.85      0.74      0.79       671
           6       0.90      0.23      0.36       798

    accuracy                           0.64      4436
   macro avg       0.70      0.64      0.62      4436
weighted avg       0.71      0.64      0.63      4436

2025-01-29 20:38:04,954 - INFO - numLayer1_hiddenSize128_seq_num80_gap3_accuracy88.09, total samples: 4431
2025-01-29 20:38:04,954 - INFO - 
 [[404 127  34   0  11   0]
 [ 86 638   0   0   0   0]
 [ 47  35 704  40 174  12]
 [ 57 143  21 435   1   0]
 [  4   0 101  36 518   2]
 [223 364   0   8   0 206]]
2025-01-29 20:38:04,961 - INFO - 
               precision    recall  f1-score   support

           1       0.49      0.70      0.58       576
           2       0.49      0.88      0.63       724
           3       0.82      0.70      0.75      1012
           4       0.84      0.66      0.74       657
           5       0.74      0.78      0.76       661
           6       0.94      0.26      0.40       801

    accuracy                           0.66      4431
   macro avg       0.72      0.66      0.64      4431
weighted avg       0.73      0.66      0.65      4431

2025-01-29 20:38:04,965 - INFO - numLayer1_hiddenSize128_seq_num100_gap3_accuracy88.77, total samples: 4435
2025-01-29 20:38:04,966 - INFO - 
 [[423 104  44   0   0   5]
 [ 30 657  15  11   0   0]
 [ 38  33 796  94  51   0]
 [ 52 187  31 388   0  11]
 [  0   2  63  85 519   0]
 [233 218  10   4   0 331]]
2025-01-29 20:38:04,973 - INFO - 
               precision    recall  f1-score   support

           1       0.55      0.73      0.63       576
           2       0.55      0.92      0.69       713
           3       0.83      0.79      0.81      1012
           4       0.67      0.58      0.62       669
           5       0.91      0.78      0.84       669
           6       0.95      0.42      0.58       796

    accuracy                           0.70      4435
   macro avg       0.74      0.70      0.69      4435
weighted avg       0.76      0.70      0.70      4435

2025-01-29 20:38:04,977 - INFO - numLayer1_hiddenSize128_seq_num80_gap5_accuracy85.92, total samples: 4428
2025-01-29 20:38:04,977 - INFO - 
 [[356 139  64   0   0  14]
 [ 96 621   0   3   0   3]
 [ 54 103 762  33  27  29]
 [ 72 202  55 324   0   9]
 [  9   6 125 126 400   0]
 [219 366   0   0   0 211]]
2025-01-29 20:38:04,983 - INFO - 
               precision    recall  f1-score   support

           1       0.44      0.62      0.52       573
           2       0.43      0.86      0.57       723
           3       0.76      0.76      0.76      1008
           4       0.67      0.49      0.56       662
           5       0.94      0.60      0.73       666
           6       0.79      0.27      0.40       796

    accuracy                           0.60      4428
   macro avg       0.67      0.60      0.59      4428
weighted avg       0.68      0.60      0.60      4428

2025-01-29 20:38:04,988 - INFO - numLayer1_hiddenSize128_seq_num100_gap5_accuracy85.73, total samples: 4436
2025-01-29 20:38:04,988 - INFO - 
 [[354 171  51   0   0   0]
 [ 73 633   2   0   0   3]
 [ 37  21 822  54  72   8]
 [ 31  98  54 461   0  25]
 [  0   0  77  73 516   3]
 [188 274   0   3   0 332]]
2025-01-29 20:38:04,994 - INFO - 
               precision    recall  f1-score   support

           1       0.52      0.61      0.56       576
           2       0.53      0.89      0.66       711
           3       0.82      0.81      0.81      1014
           4       0.78      0.69      0.73       669
           5       0.88      0.77      0.82       669
           6       0.89      0.42      0.57       797

    accuracy                           0.70      4436
   macro avg       0.74      0.70      0.69      4436
weighted avg       0.75      0.70      0.70      4436
